import os

import streamlit as st
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_community.vectorstores.faiss import FAISS
from langchain_core.messages import ChatMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Initialize App requirements
os.makedirs(".cache", exist_ok=True)

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
OPENAI_BASE_URL = os.environ["OPENAI_BASE_URL"]
LLM_MODEL = os.environ["LLM_MODEL"]

# Initialize the state of the app
files = []
retriever = None
st.session_state["messages"] = st.session_state.get(
    "messages",
    [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"),
    ],
)

# Show title and description.
st.title("Welcome to AIEEV Chat!")
st.write(
    "Upload a document below and ask a question about it â€“ GPT will answer!  \n"
    f"Current model: {LLM_MODEL}"
)


def new_chat():
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]


with st.sidebar:
    st.logo(
        "https://aieev-public.s3.ap-northeast-2.amazonaws.com/assets/images/logos/%E1%84%8B%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%87%E1%85%B3+%E1%84%85%E1%85%A9%E1%84%80%E1%85%A9%E1%84%83%E1%85%B5%E1%84%8C%E1%85%A1%E1%84%8B%E1%85%B5%E1%86%AB-01-crop.png",
        link="https://aieev.com",
    )
    st.title("AIEEV Chat")

    uploaded_files = st.file_uploader(
        "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš” (.txt or .md or .pdf)",
        type=("txt", "md", "pdf"),
        accept_multiple_files=True,
    )

    button = st.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”")
    if button:
        new_chat()

    st.divider()

    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        value=OPENAI_API_KEY,
    )

    # Ask user for their OpenAI API key via `st.text_input`.
    # Alternatively, you can store the API key in `./.streamlit/secrets.toml` and access it
    # via `st.secrets`, see https://docs.streamlit.io/develop/concepts/connections/secrets-management
    # openai_api_key = st.text_input("OpenAI API Key", type="password")
    if not openai_api_key:
        st.info("Please add your OpenAI API key to continue.", icon="ğŸ—ï¸")

    # "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"
    # "[Homepage](https://aieev.com)"
    # "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"


def init_retriever():
    docs = []
    for file in files:
        # Step 1: ë¬¸ì„œ ë¡œë“œ
        # loader = TextLoader("data/appendix-keywords.txt")
        if file.endswith(".txt") or file.endswith(".md"):
            loader = TextLoader(file)
        elif file.endswith(".pdf"):
            loader = PyPDFLoader(file)

        # Step 2: ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # 500ìë¡œ ë¬¸ì„œë¥¼ ë¶„í• 
            chunk_overlap=50,  # 50ìì˜ ì¤‘ë³µì„ í—ˆìš©
            length_function=len,
        )
        docs += loader.load_and_split(text_splitter)

    # Step 3: ë²¡í„° ì €ì¥ì†Œ ìƒì„± & ì„ë² ë”©(ë¬¸ì¥ì„ ìˆ«ì í‘œí˜„ìœ¼ë¡œ ë°”ê¾¼ë‹¤!!) -> ì €ì¥
    vectorstore = FAISS.from_documents(
        docs,
        embedding=OpenAIEmbeddings(
            api_key=openai_api_key,  # type: ignore
            base_url=OPENAI_BASE_URL,
            # check_embedding_ctx_length=False,
        ),
    )

    # Step 4: ê²€ìƒ‰ê¸°(retriever) -> ë‚˜ì¤‘ì— ì§ˆë¬¸(Query) ì— ëŒ€í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í•˜ê¸° ìœ„í•¨
    retriever = vectorstore.as_retriever()
    return retriever


if uploaded_files:
    for uploaded_file in uploaded_files:
        bytes_data = uploaded_file.read()

        file_path = f"./.cache/{uploaded_file.name}"
        with open(file_path, "wb") as f:
            f.write(bytes_data)

        files.append(file_path)

    retriever = init_retriever()


def get_answer_stream(question: str):
    if retriever:
        # Step 5: í”„ë¡¬í”„íŠ¸ ì‘ì„±, context: ê²€ìƒ‰ê¸°ì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ì¥, question: ì§ˆë¬¸
        template = """ë‹¹ì‹ ì€ ë¬¸ì„œì— ëŒ€í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ Assistant ì…ë‹ˆë‹¤. ë¬´ì¡°ê±´, ì£¼ì–´ì§„ Context ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ë‹µë³€ì— ëŒ€í•œ ì¶œì²˜ë„ í•¨ê»˜ ì œê³µí•´ ì£¼ì„¸ìš”.
        ì¶œì²˜ëŠ” íŒŒì¼ ì´ë¦„ê³¼ í˜ì´ì§€ ë²ˆí˜¸ë¡œ í‘œê¸°í•´ ì£¼ì„¸ìš”.

        #Context:
        {context}

        #Question:
        {question}
        """

        # Step 6: OpenAI GPT-4 ëª¨ë¸ì„ ì„¤ì •
        model = ChatOpenAI(
            api_key=openai_api_key,  # type: ignore
            base_url=OPENAI_BASE_URL,
            model=LLM_MODEL,
            streaming=True,
            callbacks=[StreamingStdOutCallbackHandler()],
        )
        prompt = ChatPromptTemplate.from_template(template)

        # Step 7: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ê¸° ìœ„í•œ ì²´ì¸ ìƒì„±
        retrieval_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | model
            | StrOutputParser()
        )
        return retrieval_chain.stream(question)
    else:
        template = """ë„ˆëŠ” ì¼ë°˜ì ì¸ ë‹µë³€ì„ í•˜ëŠ” Assistantì•¼. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ í•´ì¤„ ìˆ˜ ìˆì–´. ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜.
        
        #Question:
        {question}
        """
        model = ChatOpenAI(
            api_key=openai_api_key,  # type: ignore
            base_url=OPENAI_BASE_URL,
            model=LLM_MODEL,
            streaming=True,
            # temperature=PROMPT_LIST[prompt_index]["temperature"],
            callbacks=[StreamingStdOutCallbackHandler()],
        )
        prompt = ChatPromptTemplate.from_template(template)

        # Step 8: ì§ˆë¬¸&ë‹µë³€
        chain = prompt | model | StrOutputParser()
        return chain.stream({"question": user_input})


# Print messages
for message in st.session_state["messages"]:
    st.chat_message(message.role).write(message.content)


user_input = st.chat_input(
    "ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”",
    key="user_input",
    # disabled=(not openai_api_key or not uploaded_files),
)

if user_input:
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))
    st.chat_message("user").write(user_input)

    # Generate an answer using the OpenAI API.
    with st.chat_message("assistant"):
        response_container = st.empty()
        response = ""
        for chunk in get_answer_stream(user_input):
            response += chunk
            response_container.markdown(response)

    st.session_state["messages"].append(ChatMessage(role="assistant", content=response))
